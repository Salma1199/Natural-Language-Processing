Skipping entry because it's not a dictionary: []
Skipping entry because it's not a dictionary: []
Skipping entry because it's not a dictionary: 
Train set size: 8264
Dev set size: 1033
Test set size: 1033
Top 10 largest words:
(('di', 'oc', 'ty', 'lb', 'en', 'zo', 'thi', 'eno', 'ben', 'zo', 'thi', 'op', 'hen', 'e'), 2)
(('cu', '44', 'z', 'r', '44', 'al', '8', 'h', 'f', '2', 'co', '2'), 1)
(('te', 'tra', 'met', 'hyl', 'tet', 'rase', 'lena', 'ful', 'vale', 'ne'), 1)
(('bi', '2', 'sr', '2', 'ca', 'cu', '2', 'o', '8'), 3)
(('c', '9', 'h', '18', 'n', '2', 'cu', 'br', '4'), 1)
(('rus', 'r', '2', 'g', 'dc', 'u', '2', 'o', '8'), 1)
(('ol', 'igo', 'ph', 'en', 'yle', 'ne', 'vin', 'ule', 'ne'), 1)
(('te', 'tra', 'cy', 'ano', 'quin', 'od', 'ime', 'than', 'e'), 1)
(('magnet', 'oe', 'le', 'ct', 'rol', 'umi', 'nes', 'cence'), 3)
(('la', '2', 'o', '3', 'fe', '2', 'se', '2'), 2)
100 smallest tokens:
(('cu', '44', 'z', 'r', '44', 'al', '8', 'h', 'f', '2', 'co', '2'), 1)
(('te', 'tra', 'met', 'hyl', 'tet', 'rase', 'lena', 'ful', 'vale', 'ne'), 1)
(('c', '9', 'h', '18', 'n', '2', 'cu', 'br', '4'), 1)
(('rus', 'r', '2', 'g', 'dc', 'u', '2', 'o', '8'), 1)
(('ol', 'igo', 'ph', 'en', 'yle', 'ne', 'vin', 'ule', 'ne'), 1)
(('te', 'tra', 'cy', 'ano', 'quin', 'od', 'ime', 'than', 'e'), 1)
(('na', '2', 'cu', '3', 'ge', '4', 'o', '12'), 1)
(('benz', 'od', 'ith', 'io', 'ph', 'ene', 'al', 't'), 1)
(('t', 'l', '2', 'ba', '2', 'cu', 'o', '6'), 1)
(('did', 'ode', 'cy', 'ld', 'ime', 'thy', 'lam', 'monium'), 1)
(('lu', '2', 'bi', 'fe', '4', 'ga', 'o', '12'), 1)
(('h', 'gb', 'a', '2', 'cu', 'o', '4'), 1)
(('am', 'n', '3', 'cr', '4', 'o', '12'), 1)
(('di', 'ff', 'us', 'io', 'os', 'mot', 'ic'), 1)
(('k', '3', 'ph', 'ena', 'nt', 'hre', 'ne'), 1)
(('k', 'x', 'ph', 'ena', 'nt', 'hre', 'ne'), 1)
(('ba', '3', 'm', 'ru', '2', 'o', '9'), 1)
(('ba', '3', 'fer', 'u', '2', 'o', '9'), 1)
(('x', 'na', 'x', 'fe', '2', 'as', '2'), 1)
(('cu', '2', 'hg', 'p', 'bs', 'e', '4'), 1)
(('cu', '2', 'cd', 'p', 'bs', 'e', '4'), 1)
(('ag', '2', 'cd', 'p', 'bt', 'e', '4'), 1)
(('du', '20', '15', 'un', 'sat', 'ura', 'ted'), 1)
(('bu', 'rk', 'ov', '20', '11', 'top', 'ological'), 1)
(('che', 'mo', 'hy', 'dro', 'dy', 'nami', 'c'), 1)
(('ca', '2', 'cu', 'o', '2', 'cl', '2'), 1)
(('tri', 'fl', 'uo', 'ro', 'eth', 'yle', 'ne'), 1)
(('ba', '3', 'm', 'n', '2', 'o', '8'), 1)
(('pie', 'zo', 'ma', 'gne', 'ti', 'x', 'm'), 1)
(('g', 'd', '3', 'ru', '4', 'al', '12'), 1)
(('y', 'ba', '2', 'cu', '4', 'o', '8'), 1)
(('ph', 'en', 'yle', 'nee', 'thy', 'ny', 'lene'), 1)
(('na', '2', 'co', '2', 'te', 'o', '6'), 1)
(('g', 'd', '3', 'ga', '5', 'o', '12'), 1)
(('he', 'xa', 'fl', 'uo', 'rop', 'hos', 'phate'), 1)
(('che', 'mo', 'hy', 'dro', 'dy', 'nami', 'cs'), 1)
(('ph', 'en', 'yle', 'ne', 'vin', 'yle', 'ne'), 1)
(('iso', 'pro', 'py', 'la', 'cr', 'yla', 'mide'), 1)
(('super', 'co', 'unt', 'er', 'fl', 'uid', 's'), 1)
(('h', 'f', '2', 'n', '2', 'i', '2'), 1)
(('z', 'r', '2', 'n', '2', 'cl', '2'), 1)
(('gr', 'avi', 'tom', 'agne', 'to', 'ele', 'ctric'), 1)
(('magnet', 'os', 'pe', 'ct', 'ros', 'co', 'py'), 1)
(('none', 'quil', 'ib', 'ri', 'bri', 'um'), 1)
(('electro', 'hy', 'dro', 'dy', 'nami', 'c'), 1)
(('el', 'ast', 'oca', 'pi', 'llar', 'ity'), 1)
(('anti', 'fer', 'rro', 'ma', 'gne', 'tic'), 1)
(('rn', 'i', '2', 'b', '2', 'c'), 1)
(('c1', '0', 'h', '12', 'se', '4'), 1)
(('c1', '0', 'h', '8', 's', '8'), 1)
(('anti', 'fer', 'ror', 'ota', 'tion', 'al'), 1)
(('im', 'mun', 'og', 'lo', 'bu', 'lin'), 1)
(('ca', 'k', 'fe', '4', 'as', '4'), 1)
(('he', 'xy', 'lth', 'io', 'ph', 'ene'), 1)
(('adamant', 'yl', 'car', 'ba', 'mo', 'yl'), 1)
(('sr', '3', 'cr', '2', 'o', '8'), 1)
(('anti', 'fer', 'ro', 'ele', 'ctric', 'ally'), 1)
(('stab', 'ilis', 'ation', 'me', 'chan', 'ism'), 1)
(('tunnel', 'ling', 'mic', 'ros', 'co', 'py'), 1)
(('te', 'tra', 'car', 'box', 'yl', 'ic'), 1)
(('the', 'rm', 'ov', 'is', 'coe', 'lastic'), 1)
(('dial', 'ly', 'ld', 'ime', 'thy', 'l'), 1)
(('di', 'sp', 'rop', 'ort', 'ion', 'ate'), 1)
(('ra', 'di', 'ii', 'nde', 'pen', 'dent'), 1)
(('magnet', 'op', 'ie', 'zo', 'ele', 'ctric'), 1)
(('ph', 'yt', 'op', 'lan', 'kt', 'on'), 1)
(('xu', '20', '15', 'dis', 'co', 'very'), 1)
(('wen', 'g', '20', '15', 'we', 'yl'), 1)
(('xu', '20', '15', 'ob', 'ser', 'vation'), 1)
(('liang', '20', '15', 'ult', 'rah', 'igh'), 1)
(('she', 'khar', '20', '15', 'lar', 'ge'), 1)
(('xi', 'e', '20', '15', 'ne', 'w'), 1)
(('r', 'him', '20', '15', 'landa', 'u'), 1)
(('un', 're', 'con', 'st', 'ru', 'cted'), 1)
(('os', 'te', 'oa', 'rth', 'rit', 'is'), 1)
(('co', '40', 'fe', '40', 'b', '20'), 1)
(('ci', 'rc', 'um', 'cor', 'one', 'ne'), 1)
(('ox', 'ych', 'al', 'co', 'gen', 'ide'), 1)
(('vinyl', 'ide', 'ne', 'fl', 'uo', 'ride'), 1)
(('dime', 'thy', 'lp', 'ipe', 'raz', 'ine'), 1)
(('x', 'sr', 'x', 'm', 'no', '3'), 1)
(('n', 'd', '2', 'fe', '14', 'b'), 1)
(('al', 'kan', 'ed', 'ith', 'iol', 's'), 1)
(('ion', 'yl', 'ide', 'nea', 'ce', 'tic'), 1)
(('wal', 'dh', 'er', 'r', '20', '14'), 1)
(('ve', 'ld', 'hor', 'st', '20', '14'), 1)
(('right', 'le', 'ft', 'har', 'poo', 'ns'), 1)
(('magnet', 'ost', 'ru', 'ct', 'ural', 'ly'), 1)
(('poly', 'ac', 'ryn', 'oni', 'tri', 'le'), 1)
(('ol', 'igo', 'sa', 'cc', 'hari', 'des'), 1)
(('cai', '20', '15', 'vis', 'ual', 'izing'), 1)
(('ye', '20', '13', 'vis', 'ual', 'izing'), 1)
(('millie', 'le', 'ct', 'ron', 'vo', 'lt'), 1)
(('fe', '2', 'mo', '3', 'o', '8'), 1)
(('stu', 'che', 'br', 'uk', 'ho', 'v'), 1)
(('stu', 'che', 'br', 'uk', 'ho', 'va'), 1)
(('9', 'ca', 'cu', '2', 'o', '8'), 1)
(('the', 'rm', 'oe', 'le', 'ctric', 'ity'), 1)
(('ac', 'ous', 'to', 'dy', 'nami', 'cs'), 1)
(('300', 'x', '30', '0', 'um', '2'), 1)
Iteration: Learning Rate: 1e-05, Epochs: 1, Batch Size: 16
Average loss for epoch 1: 2.0531
Average loss on test data: 1.9231
              precision    recall  f1-score   support

           0       0.67      0.01      0.03       143
           1       0.00      0.00      0.00        60
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       151
           4       0.24      0.71      0.36       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.00      0.00      0.00       110
           8       0.41      0.90      0.56       184

    accuracy                           0.31      1033
   macro avg       0.15      0.18      0.11      1033
weighted avg       0.21      0.31      0.18      1033

Iteration: Learning Rate: 1e-05, Epochs: 1, Batch Size: 32
Average loss for epoch 1: 2.0996
Average loss on test data: 1.9612
              precision    recall  f1-score   support

           0       0.54      0.55      0.55       143
           1       0.00      0.00      0.00        60
           2       0.00      0.00      0.00       100
           3       0.00      0.00      0.00       151
           4       0.26      0.94      0.41       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.00      0.00      0.00       110
           8       0.33      0.24      0.28       184

    accuracy                           0.31      1033
   macro avg       0.13      0.19      0.14      1033
weighted avg       0.19      0.31      0.21      1033

Iteration: Learning Rate: 1e-05, Epochs: 2, Batch Size: 16
Average loss for epoch 1: 2.0630
Average loss for epoch 2: 1.7507
Average loss on test data: 1.5131
              precision    recall  f1-score   support

           0       0.65      0.88      0.75       143
           1       0.00      0.00      0.00        60
           2       0.87      0.13      0.23       100
           3       0.42      0.68      0.52       151
           4       0.55      0.65      0.59       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.90      0.08      0.15       110
           8       0.55      0.95      0.70       184

    accuracy                           0.55      1033
   macro avg       0.44      0.38      0.33      1033
weighted avg       0.54      0.55      0.46      1033

Iteration: Learning Rate: 1e-05, Epochs: 2, Batch Size: 32
Average loss for epoch 1: 2.1368
Average loss for epoch 2: 1.9841
Average loss on test data: 1.8264
              precision    recall  f1-score   support

           0       0.77      0.41      0.53       143
           1       1.00      0.02      0.03        60
           2       0.23      0.03      0.05       100
           3       0.31      0.16      0.21       151
           4       0.30      0.95      0.45       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.00      0.00      0.00       110
           8       0.72      0.74      0.73       184

    accuracy                           0.41      1033
   macro avg       0.37      0.26      0.22      1033
weighted avg       0.42      0.41      0.33      1033

Iteration: Learning Rate: 1e-05, Epochs: 3, Batch Size: 16
Average loss for epoch 1: 2.0918
Average loss for epoch 2: 1.8462
Average loss for epoch 3: 1.5207
Average loss on test data: 1.5370
              precision    recall  f1-score   support

           0       0.67      0.67      0.67       143
           1       0.00      0.00      0.00        60
           2       0.20      0.05      0.08       100
           3       0.28      0.10      0.15       151
           4       0.36      0.94      0.52       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.50      0.05      0.08       110
           8       0.65      0.88      0.75       184

    accuracy                           0.46      1033
   macro avg       0.30      0.30      0.25      1033
weighted avg       0.40      0.46      0.37      1033

Iteration: Learning Rate: 1e-05, Epochs: 3, Batch Size: 32
Average loss for epoch 1: 2.1094
Average loss for epoch 2: 1.9384
Average loss for epoch 3: 1.7024
Average loss on test data: 1.5911
              precision    recall  f1-score   support

           0       0.62      0.88      0.73       143
           1       0.00      0.00      0.00        60
           2       0.61      0.14      0.23       100
           3       0.50      0.40      0.44       151
           4       0.44      0.86      0.58       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.38      0.05      0.08       110
           8       0.61      0.86      0.71       184

    accuracy                           0.53      1033
   macro avg       0.35      0.35      0.31      1033
weighted avg       0.46      0.53      0.44      1033

Iteration: Learning Rate: 5e-05, Epochs: 1, Batch Size: 16
Average loss for epoch 1: 1.9228
Average loss on test data: 1.6064
              precision    recall  f1-score   support

           0       0.59      0.87      0.70       143
           1       0.00      0.00      0.00        60
           2       0.33      0.01      0.02       100
           3       0.75      0.04      0.08       151
           4       0.31      0.93      0.46       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.11      0.01      0.02       110
           8       0.69      0.62      0.65       184

    accuracy                           0.43      1033
   macro avg       0.31      0.27      0.21      1033
weighted avg       0.42      0.43      0.32      1033

Iteration: Learning Rate: 5e-05, Epochs: 1, Batch Size: 32
Average loss for epoch 1: 2.0432
Average loss on test data: 1.8402
              precision    recall  f1-score   support

           0       0.70      0.71      0.70       143
           1       0.00      0.00      0.00        60
           2       0.00      0.00      0.00       100
           3       0.14      0.01      0.02       151
           4       0.33      0.66      0.44       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.00      0.00      0.00       110
           8       0.39      0.94      0.55       184

    accuracy                           0.40      1033
   macro avg       0.17      0.26      0.19      1033
weighted avg       0.25      0.40      0.29      1033

Iteration: Learning Rate: 5e-05, Epochs: 2, Batch Size: 16
Average loss for epoch 1: 1.9866
Average loss for epoch 2: 1.5089
Average loss on test data: 1.3635
              precision    recall  f1-score   support

           0       0.69      0.84      0.75       143
           1       0.73      0.67      0.70        60
           2       0.40      0.62      0.48       100
           3       0.44      0.39      0.41       151
           4       0.66      0.46      0.54       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.43      0.16      0.24       110
           8       0.53      0.93      0.67       184

    accuracy                           0.55      1033
   macro avg       0.43      0.45      0.42      1033
weighted avg       0.52      0.55      0.51      1033

Iteration: Learning Rate: 5e-05, Epochs: 2, Batch Size: 32
Average loss for epoch 1: 2.0231
Average loss for epoch 2: 1.5330
Average loss on test data: 1.3969
              precision    recall  f1-score   support

           0       0.64      0.83      0.72       143
           1       0.92      0.20      0.33        60
           2       0.39      0.61      0.48       100
           3       0.44      0.59      0.50       151
           4       0.64      0.65      0.64       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.44      0.15      0.23       110
           8       0.70      0.84      0.76       184

    accuracy                           0.57      1033
   macro avg       0.46      0.43      0.41      1033
weighted avg       0.55      0.57      0.53      1033

Iteration: Learning Rate: 5e-05, Epochs: 3, Batch Size: 16
Average loss for epoch 1: 1.9521
Average loss for epoch 2: 1.4569
Average loss for epoch 3: 1.1676
Average loss on test data: 1.2943
              precision    recall  f1-score   support

           0       0.64      0.84      0.73       143
           1       0.76      0.62      0.68        60
           2       0.57      0.47      0.51       100
           3       0.54      0.44      0.48       151
           4       0.61      0.79      0.69       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.35      0.39      0.37       110
           8       0.74      0.78      0.76       184

    accuracy                           0.60      1033
   macro avg       0.47      0.48      0.47      1033
weighted avg       0.56      0.60      0.58      1033

Iteration: Learning Rate: 5e-05, Epochs: 3, Batch Size: 32
Average loss for epoch 1: 2.0444
Average loss for epoch 2: 1.5201
Average loss for epoch 3: 1.1180
Average loss on test data: 1.3404
              precision    recall  f1-score   support

           0       0.65      0.82      0.73       143
           1       0.00      0.00      0.00        60
           2       0.48      0.63      0.54       100
           3       0.46      0.58      0.51       151
           4       0.59      0.76      0.67       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.37      0.15      0.22       110
           8       0.74      0.88      0.80       184

    accuracy                           0.59      1033
   macro avg       0.37      0.42      0.39      1033
weighted avg       0.50      0.59      0.53      1033

Iteration: Learning Rate: 3e-05, Epochs: 1, Batch Size: 16
Average loss for epoch 1: 1.8974
Average loss on test data: 1.5864
              precision    recall  f1-score   support

           0       0.52      0.94      0.67       143
           1       0.67      0.50      0.57        60
           2       0.39      0.54      0.45       100
           3       0.00      0.00      0.00       151
           4       0.44      0.76      0.56       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.46      0.10      0.16       110
           8       0.65      0.70      0.67       184

    accuracy                           0.50      1033
   macro avg       0.35      0.39      0.34      1033
weighted avg       0.40      0.50      0.42      1033

Iteration: Learning Rate: 3e-05, Epochs: 1, Batch Size: 32
Average loss for epoch 1: 2.0451
Average loss on test data: 1.8497
              precision    recall  f1-score   support

           0       0.66      0.61      0.64       143
           1       0.00      0.00      0.00        60
           2       0.00      0.00      0.00       100
           3       0.41      0.32      0.36       151
           4       0.30      0.90      0.45       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.00      0.00      0.00       110
           8       0.48      0.40      0.44       184

    accuracy                           0.39      1033
   macro avg       0.21      0.25      0.21      1033
weighted avg       0.30      0.39      0.31      1033

Iteration: Learning Rate: 3e-05, Epochs: 2, Batch Size: 16
Average loss for epoch 1: 1.9770
Average loss for epoch 2: 1.4600
Average loss on test data: 1.3537
              precision    recall  f1-score   support

           0       0.66      0.81      0.73       143
           1       0.59      0.78      0.67        60
           2       0.45      0.48      0.47       100
           3       0.48      0.67      0.56       151
           4       0.59      0.59      0.59       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.43      0.08      0.14       110
           8       0.72      0.90      0.80       184

    accuracy                           0.59      1033
   macro avg       0.44      0.48      0.44      1033
weighted avg       0.53      0.59      0.54      1033

Iteration: Learning Rate: 3e-05, Epochs: 2, Batch Size: 32
Average loss for epoch 1: 2.0154
Average loss for epoch 2: 1.6284
Average loss on test data: 1.3842
              precision    recall  f1-score   support

           0       0.71      0.76      0.73       143
           1       0.70      0.50      0.58        60
           2       0.57      0.47      0.52       100
           3       0.57      0.19      0.28       151
           4       0.55      0.84      0.67       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.39      0.37      0.38       110
           8       0.61      0.92      0.73       184

    accuracy                           0.58      1033
   macro avg       0.45      0.45      0.43      1033
weighted avg       0.54      0.58      0.53      1033

Iteration: Learning Rate: 3e-05, Epochs: 3, Batch Size: 16
Average loss for epoch 1: 1.9065
Average loss for epoch 2: 1.3828
Average loss for epoch 3: 1.0341
Average loss on test data: 1.2752
              precision    recall  f1-score   support

           0       0.76      0.78      0.77       143
           1       0.78      0.12      0.20        60
           2       0.47      0.68      0.56       100
           3       0.46      0.44      0.45       151
           4       0.63      0.80      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.39      0.42      0.41       110
           8       0.75      0.82      0.78       184

    accuracy                           0.60      1033
   macro avg       0.47      0.45      0.43      1033
weighted avg       0.57      0.60      0.56      1033

Iteration: Learning Rate: 3e-05, Epochs: 3, Batch Size: 32
Average loss for epoch 1: 2.0660
Average loss for epoch 2: 1.6693
Average loss for epoch 3: 1.2507
Average loss on test data: 1.2596
              precision    recall  f1-score   support

           0       0.68      0.85      0.76       143
           1       0.87      0.43      0.58        60
           2       0.57      0.40      0.47       100
           3       0.54      0.44      0.48       151
           4       0.51      0.80      0.63       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.41      0.26      0.32       110
           8       0.71      0.90      0.79       184

    accuracy                           0.60      1033
   macro avg       0.48      0.45      0.45      1033
weighted avg       0.55      0.60      0.56      1033

Best hyperparameters: {'learning_rate': 3e-05, 'epochs': 3, 'batch_size': 32}
Average loss for epoch 1: 1.5330
Average loss for epoch 2: 1.0292
Average loss for epoch 3: 0.8317
Average loss on test data: 1.0063
              precision    recall  f1-score   support

           0       0.73      0.89      0.80       129
           1       0.81      0.68      0.74        81
           2       0.57      0.70      0.63       104
           3       0.63      0.69      0.66       130
           4       0.73      0.67      0.70       218
           5       1.00      0.05      0.10        39
           6       0.00      0.00      0.00        48
           7       0.48      0.45      0.47        93
           8       0.70      0.91      0.79       191

    accuracy                           0.68      1033
   macro avg       0.63      0.56      0.54      1033
weighted avg       0.66      0.68      0.65      1033

Top 10 largest words:
(('di', 'oct', 'yl', 'benz', 'othi', 'eno', 'benz', 'othi', 'ophen', 'e'), 2)
(('cu', '44', 'zr', '44', 'al', '8', 'hf', '2', 'co', '2'), 1)
(('bi', '2', 'sr', '2', 'cac', 'u', '2', 'o', '8'), 3)
(('tetra', 'methyl', 'tetr', 'ase', 'len', 'af', 'ul', 'val', 'ene'), 1)
(('c', '9', 'h', '18', 'n', '2', 'cu', 'br', '4'), 1)
(('la', '2', 'o', '3', 'fe', '2', 'se', '2'), 2)
(('na', '2', 'cu', '3', 'ge', '4', 'o', '12'), 1)
(('did', 'ode', 'cy', 'ld', 'ime', 'thy', 'lamm', 'onium'), 1)
(('lu', '2', 'bi', 'fe', '4', 'ga', 'o', '12'), 1)
(('rus', 'r', '2', 'gd', 'cu', '2', 'o', '8'), 1)
100 smallest tokens:
(('cu', '44', 'zr', '44', 'al', '8', 'hf', '2', 'co', '2'), 1)
(('tetra', 'methyl', 'tetr', 'ase', 'len', 'af', 'ul', 'val', 'ene'), 1)
(('c', '9', 'h', '18', 'n', '2', 'cu', 'br', '4'), 1)
(('na', '2', 'cu', '3', 'ge', '4', 'o', '12'), 1)
(('did', 'ode', 'cy', 'ld', 'ime', 'thy', 'lamm', 'onium'), 1)
(('lu', '2', 'bi', 'fe', '4', 'ga', 'o', '12'), 1)
(('rus', 'r', '2', 'gd', 'cu', '2', 'o', '8'), 1)
(('ba', '3', 'mr', 'u', '2', 'o', '9'), 1)
(('ba', '3', 'fer', 'u', '2', 'o', '9'), 1)
(('ca', '2', 'cu', 'o', '2', 'cl', '2'), 1)
(('yb', 'a', '2', 'cu', '4', 'o', '8'), 1)
(('tl', '2', 'ba', '2', 'cu', 'o', '6'), 1)
(('na', '2', 'co', '2', 'te', 'o', '6'), 1)
(('tetra', 'cy', 'ano', 'quin', 'odi', 'meth', 'ane'), 1)
(('anti', 'fer', 'rr', 'oma', 'gn', 'etic', 'ally'), 1)
(('none', 'qu', 'ili', 'bri', 'bri', 'um'), 1)
(('hg', 'ba', '2', 'cu', 'o', '4'), 1)
(('amn', '3', 'cr', '4', 'o', '12'), 1)
(('anti', 'fer', 'rr', 'oma', 'gn', 'etic'), 1)
(('rn', 'i', '2', 'b', '2', 'c'), 1)
(('benzo', 'di', 'thi', 'ophen', 'ea', 'lt'), 1)
(('c', '10', 'h', '12', 'se', '4'), 1)
(('c', '10', 'h', '8', 's', '8'), 1)
(('ca', 'kf', 'e', '4', 'as', '4'), 1)
(('adam', 'ant', 'yl', 'carb', 'amo', 'yl'), 1)
(('xn', 'ax', 'fe', '2', 'as', '2'), 1)
(('sr', '3', 'cr', '2', 'o', '8'), 1)
(('cu', '2', 'hg', 'pbs', 'e', '4'), 1)
(('cu', '2', 'cd', 'pbs', 'e', '4'), 1)
(('ag', '2', 'cd', 'pb', 'te', '4'), 1)
(('bor', 'isen', 'ko', '201', '5', 'time'), 1)
(('wen', 'g', '201', '5', 'we', 'yl'), 1)
(('liang', '201', '5', 'ult', 'ra', 'high'), 1)
(('she', 'kh', 'ar', '201', '5', 'large'), 1)
(('burk', 'ov', '201', '1', 'top', 'ological'), 1)
(('rh', 'im', '201', '5', 'land', 'au'), 1)
(('co', '40', 'fe', '40', 'b', '20'), 1)
(('non', 'mon', 'oto', 'no', 'us', 'ly'), 1)
(('ba', '2', 'yr', 'e', 'o', '6'), 1)
(('nond', 'ia', 'gon', 'ali', 'za', 'ble'), 1)
(('poly', 'dia', 'ce', 'ty', 'len', 'e'), 1)
(('vel', 'dh', 'ors', 't', '201', '4'), 1)
(('ba', '3', 'mn', '2', 'o', '8'), 1)
(('gd', '3', 'ru', '4', 'al', '12'), 1)
(('mill', 'iele', 'ct', 'ron', 'vol', 't'), 1)
(('pent', 'ate', 'tel', 'lu', 'rid', 'e'), 1)
(('fe', '2', 'mo', '3', 'o', '8'), 1)
(('stu', 'che', 'br', 'uk', 'ho', 'v'), 1)
(('stu', 'che', 'br', 'uk', 'ho', 'va'), 1)
(('9', 'cac', 'u', '2', 'o', '8'), 1)
(('bi', '4', 'br', '2', 'i', '2'), 1)
(('buck', 'min', 'ster', 'full', 'eren', 'e'), 1)
(('gd', '3', 'ga', '5', 'o', '12'), 1)
(('oligo', 'phenyl', 'ene', 'vin', 'ule', 'ne'), 1)
(('isop', 'ropy', 'lac', 'ry', 'lam', 'ide'), 1)
(('nons', 'up', 'erc', 'ond', 'uct', 'ing'), 1)
(('hf', '2', 'n', '2', 'i', '2'), 1)
(('zr', '2', 'n', '2', 'cl', '2'), 1)
(('cu', '3', 'v', '2', 'o', '7'), 1)
(('una', 'ni', 'mo', 'us', 'ly'), 1)
(('iso', 'cho', 'resp', 'lane', 's'), 1)
(('elast', 'oca', 'pi', 'll', 'arity'), 1)
(('ru', 'zs', 'ins', 'zk', 'y'), 1)
(('piez', 'oma', 'gn', 'eti', 'sm'), 1)
(('ultra', 'inc', 'omp', 'ress', 'ible'), 1)
(('rm', 'n', '2', 'o', '5'), 1)
(('ca', '2', 'ru', 'o', '4'), 1)
(('k', '3', 'phen', 'anth', 'rene'), 1)
(('anti', 'fer', 'ror', 'otation', 'al'), 1)
(('dich', 'alo', 'co', 'gen', 'ides'), 1)
(('act', 'eta', 'ni', 'li', 'de'), 1)
(('anti', 'fer', 'roma', 'get', 'ic'), 1)
(('gin', 'z', 'burg', 'land', 'au'), 1)
(('per', 'pend', 'ic', 'lu', 'ar'), 1)
(('hex', 'yl', 'thi', 'ophen', 'e'), 1)
(('bio', 'rt', 'hog', 'onal', 'ity'), 1)
(('coc', 'r', '2', 'o', '4'), 1)
(('matrix', 'reno', 'rm', 'ali', 'zation'), 1)
(('self', 'cons', 'iste', 'nc', 'y'), 1)
(('anti', 'fer', 'ro', 'electric', 'ally'), 1)
(('tunnel', 'ling', 'micro', 'sc', 'opy'), 1)
(('et', 'ting', 'sha', 'use', 'n'), 1)
(('biom', 'acr', 'omo', 'lec', 'ule'), 1)
(('ter', 'ep', 'ht', 'hal', 'amide'), 1)
(('va', 'ik', 'unt', 'ana', 'than'), 1)
(('intra', 'oct', 'ah', 'ed', 'ral'), 1)
(('nano', 'lit', 'hog', 'raph', 'y'), 1)
(('thc', 'r', '2', 'si', '2'), 1)
(('magnet', 'op', 'ie', 'zo', 'electric'), 1)
(('ca', 'al', '2', 'si', '2'), 1)
(('cu', 'gl', 'ian', 'do', 'lo'), 1)
(('xu', '201', '5', 'disc', 'overy'), 1)
(('xu', '201', '5', 'obs', 'ervation'), 1)
(('young', '201', '5', 'dir', 'ac'), 1)
(('gu', 'gg', 'enh', 'ei', 'm'), 1)
(('magnet', 'oe', 'xc', 'iton', 'ic'), 1)
(('superc', 'on', 'ud', 'uct', 'ing'), 1)
(('gi', 'ova', 'mb', 'att', 'ista'), 1)
(('poly', 'amo', 'rp', 'his', 'm'), 1)
(('eur', 'h', '2', 'si', '2'), 1)
Iteration: Learning Rate: 1e-05, Epochs: 1, Batch Size: 16
Average loss for epoch 1: 1.9308
Average loss on test data: 1.6056
              precision    recall  f1-score   support

           0       0.61      0.90      0.72       143
           1       0.51      0.63      0.57        60
           2       0.67      0.06      0.11       100
           3       0.42      0.31      0.36       151
           4       0.57      0.77      0.65       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.60      0.03      0.05       110
           8       0.53      0.96      0.68       184

    accuracy                           0.54      1033
   macro avg       0.43      0.41      0.35      1033
weighted avg       0.51      0.54      0.46      1033

Iteration: Learning Rate: 1e-05, Epochs: 1, Batch Size: 32
Average loss for epoch 1: 2.0319
Average loss on test data: 1.8365
              precision    recall  f1-score   support

           0       0.64      0.88      0.74       143
           1       0.00      0.00      0.00        60
           2       0.75      0.09      0.16       100
           3       0.00      0.00      0.00       151
           4       0.33      0.94      0.49       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.00      0.00      0.00       110
           8       0.43      0.51      0.47       184

    accuracy                           0.41      1033
   macro avg       0.24      0.27      0.21      1033
weighted avg       0.30      0.41      0.30      1033

Iteration: Learning Rate: 1e-05, Epochs: 2, Batch Size: 16
Average loss for epoch 1: 1.9342
Average loss for epoch 2: 1.4540
Average loss on test data: 1.2656
              precision    recall  f1-score   support

           0       0.75      0.81      0.78       143
           1       0.70      0.63      0.67        60
           2       0.66      0.47      0.55       100
           3       0.60      0.49      0.54       151
           4       0.61      0.81      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.50      0.52      0.51       110
           8       0.72      0.92      0.81       184

    accuracy                           0.65      1033
   macro avg       0.50      0.52      0.50      1033
weighted avg       0.60      0.65      0.62      1033

Iteration: Learning Rate: 1e-05, Epochs: 2, Batch Size: 32
Average loss for epoch 1: 2.0155
Average loss for epoch 2: 1.6264
Average loss on test data: 1.4018
              precision    recall  f1-score   support

           0       0.74      0.83      0.78       143
           1       0.88      0.48      0.62        60
           2       0.52      0.57      0.54       100
           3       0.64      0.34      0.45       151
           4       0.59      0.84      0.69       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.53      0.38      0.44       110
           8       0.63      0.91      0.75       184

    accuracy                           0.62      1033
   macro avg       0.50      0.48      0.48      1033
weighted avg       0.59      0.62      0.58      1033

Iteration: Learning Rate: 1e-05, Epochs: 3, Batch Size: 16
Average loss for epoch 1: 1.9016
Average loss for epoch 2: 1.3544
Average loss for epoch 3: 1.0772
Average loss on test data: 1.1365
              precision    recall  f1-score   support

           0       0.79      0.78      0.78       143
           1       0.69      0.72      0.70        60
           2       0.51      0.81      0.63       100
           3       0.66      0.50      0.57       151
           4       0.67      0.67      0.67       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.46      0.60      0.52       110
           8       0.75      0.84      0.79       184

    accuracy                           0.65      1033
   macro avg       0.50      0.55      0.52      1033
weighted avg       0.62      0.65      0.63      1033

Iteration: Learning Rate: 1e-05, Epochs: 3, Batch Size: 32
Average loss for epoch 1: 2.0371
Average loss for epoch 2: 1.6429
Average loss for epoch 3: 1.3230
Average loss on test data: 1.2561
              precision    recall  f1-score   support

           0       0.73      0.82      0.77       143
           1       0.63      0.73      0.68        60
           2       0.69      0.37      0.48       100
           3       0.54      0.65      0.59       151
           4       0.68      0.72      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.49      0.42      0.45       110
           8       0.69      0.94      0.79       184

    accuracy                           0.64      1033
   macro avg       0.49      0.52      0.50      1033
weighted avg       0.60      0.64      0.61      1033

Iteration: Learning Rate: 5e-05, Epochs: 1, Batch Size: 16
Average loss for epoch 1: 1.6073
Average loss on test data: 1.2736
              precision    recall  f1-score   support

           0       0.66      0.91      0.76       143
           1       0.69      0.73      0.71        60
           2       0.61      0.11      0.19       100
           3       0.66      0.31      0.42       151
           4       0.52      0.86      0.65       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.46      0.16      0.24       110
           8       0.57      0.91      0.70       184

    accuracy                           0.58      1033
   macro avg       0.46      0.44      0.41      1033
weighted avg       0.54      0.58      0.51      1033

Iteration: Learning Rate: 5e-05, Epochs: 1, Batch Size: 32
Average loss for epoch 1: 1.7569
Average loss on test data: 1.3103
              precision    recall  f1-score   support

           0       0.65      0.91      0.76       143
           1       0.55      0.70      0.61        60
           2       0.62      0.49      0.55       100
           3       0.62      0.26      0.37       151
           4       0.60      0.83      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.50      0.03      0.05       110
           8       0.55      0.94      0.69       184

    accuracy                           0.59      1033
   macro avg       0.45      0.46      0.41      1033
weighted avg       0.55      0.59      0.52      1033

Iteration: Learning Rate: 5e-05, Epochs: 2, Batch Size: 16
Average loss for epoch 1: 1.6622
Average loss for epoch 2: 1.1452
Average loss on test data: 1.0923
              precision    recall  f1-score   support

           0       0.68      0.87      0.77       143
           1       0.74      0.72      0.73        60
           2       0.61      0.57      0.59       100
           3       0.61      0.54      0.57       151
           4       0.69      0.72      0.71       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.46      0.51      0.48       110
           8       0.73      0.88      0.80       184

    accuracy                           0.65      1033
   macro avg       0.50      0.53      0.52      1033
weighted avg       0.61      0.65      0.63      1033

Iteration: Learning Rate: 5e-05, Epochs: 2, Batch Size: 32
Average loss for epoch 1: 1.7815
Average loss for epoch 2: 1.2248
Average loss on test data: 1.2406
              precision    recall  f1-score   support

           0       0.93      0.39      0.55       143
           1       0.72      0.63      0.67        60
           2       0.68      0.27      0.39       100
           3       0.55      0.57      0.56       151
           4       0.68      0.72      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.37      0.47      0.41       110
           8       0.51      0.98      0.67       184

    accuracy                           0.57      1033
   macro avg       0.49      0.45      0.44      1033
weighted avg       0.58      0.57      0.54      1033

Iteration: Learning Rate: 5e-05, Epochs: 3, Batch Size: 16
Average loss for epoch 1: 1.6437
Average loss for epoch 2: 1.1405
Average loss for epoch 3: 0.9082
Average loss on test data: 1.1207
              precision    recall  f1-score   support

           0       0.72      0.85      0.78       143
           1       0.73      0.67      0.70        60
           2       0.56      0.64      0.60       100
           3       0.65      0.53      0.58       151
           4       0.65      0.80      0.71       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.43      0.47      0.45       110
           8       0.75      0.78      0.77       184

    accuracy                           0.65      1033
   macro avg       0.50      0.53      0.51      1033
weighted avg       0.60      0.65      0.62      1033

Iteration: Learning Rate: 5e-05, Epochs: 3, Batch Size: 32
Average loss for epoch 1: 1.6802
Average loss for epoch 2: 1.1438
Average loss for epoch 3: 0.8648
Average loss on test data: 1.1427
              precision    recall  f1-score   support

           0       0.84      0.62      0.71       143
           1       0.64      0.78      0.71        60
           2       0.61      0.69      0.65       100
           3       0.65      0.61      0.63       151
           4       0.68      0.73      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.41      0.45      0.43       110
           8       0.65      0.89      0.75       184

    accuracy                           0.64      1033
   macro avg       0.50      0.53      0.51      1033
weighted avg       0.61      0.64      0.62      1033

Iteration: Learning Rate: 3e-05, Epochs: 1, Batch Size: 16
Average loss for epoch 1: 1.8487
Average loss on test data: 1.3794
              precision    recall  f1-score   support

           0       0.57      0.92      0.70       143
           1       1.00      0.15      0.26        60
           2       0.62      0.08      0.14       100
           3       0.43      0.43      0.43       151
           4       0.46      0.91      0.61       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.40      0.04      0.07       110
           8       0.76      0.83      0.79       184

    accuracy                           0.54      1033
   macro avg       0.47      0.37      0.33      1033
weighted avg       0.53      0.54      0.46      1033

Iteration: Learning Rate: 3e-05, Epochs: 1, Batch Size: 32
Average loss for epoch 1: 1.8961
Average loss on test data: 1.4453
              precision    recall  f1-score   support

           0       0.53      0.88      0.66       143
           1       0.71      0.68      0.69        60
           2       0.52      0.28      0.36       100
           3       0.67      0.01      0.03       151
           4       0.41      0.91      0.56       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.00      0.00      0.00       110
           8       0.72      0.84      0.78       184

    accuracy                           0.53      1033
   macro avg       0.40      0.40      0.34      1033
weighted avg       0.48      0.53      0.42      1033

Iteration: Learning Rate: 3e-05, Epochs: 2, Batch Size: 16
Average loss for epoch 1: 1.6989
Average loss for epoch 2: 1.1338
Average loss on test data: 1.0978
              precision    recall  f1-score   support

           0       0.70      0.86      0.77       143
           1       0.66      0.72      0.69        60
           2       0.50      0.74      0.59       100
           3       0.70      0.52      0.60       151
           4       0.70      0.70      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.45      0.40      0.42       110
           8       0.74      0.89      0.80       184

    accuracy                           0.65      1033
   macro avg       0.49      0.54      0.51      1033
weighted avg       0.61      0.65      0.62      1033

Iteration: Learning Rate: 3e-05, Epochs: 2, Batch Size: 32
Average loss for epoch 1: 1.8255
Average loss for epoch 2: 1.2451
Average loss on test data: 1.2254
              precision    recall  f1-score   support

           0       0.79      0.74      0.77       143
           1       0.67      0.73      0.70        60
           2       0.50      0.51      0.51       100
           3       0.57      0.54      0.55       151
           4       0.69      0.75      0.72       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.54      0.25      0.34       110
           8       0.56      0.95      0.71       184

    accuracy                           0.62      1033
   macro avg       0.48      0.50      0.48      1033
weighted avg       0.58      0.62      0.59      1033

Iteration: Learning Rate: 3e-05, Epochs: 3, Batch Size: 16
Average loss for epoch 1: 1.7017
Average loss for epoch 2: 1.1250
Average loss for epoch 3: 0.8468
Average loss on test data: 1.1096
              precision    recall  f1-score   support

           0       0.71      0.81      0.76       143
           1       0.67      0.75      0.71        60
           2       0.65      0.58      0.61       100
           3       0.58      0.65      0.61       151
           4       0.62      0.80      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.48      0.36      0.41       110
           8       0.77      0.78      0.78       184

    accuracy                           0.65      1033
   macro avg       0.50      0.53      0.51      1033
weighted avg       0.60      0.65      0.62      1033

Iteration: Learning Rate: 3e-05, Epochs: 3, Batch Size: 32
Average loss for epoch 1: 1.8589
Average loss for epoch 2: 1.2553
Average loss for epoch 3: 0.9682
Average loss on test data: 1.1075
              precision    recall  f1-score   support

           0       0.69      0.91      0.78       143
           1       0.63      0.68      0.66        60
           2       0.61      0.54      0.57       100
           3       0.67      0.62      0.64       151
           4       0.62      0.79      0.70       211
           5       0.00      0.00      0.00        37
           6       0.00      0.00      0.00        37
           7       0.46      0.31      0.37       110
           8       0.75      0.87      0.81       184

    accuracy                           0.66      1033
   macro avg       0.49      0.52      0.50      1033
weighted avg       0.60      0.66      0.62      1033

Best hyperparameters: {'learning_rate': 5e-05, 'epochs': 2, 'batch_size': 16}
Average loss for epoch 1: 1.2850
Average loss for epoch 2: 0.9318
Average loss on test data: 0.9639
              precision    recall  f1-score   support

           0       0.69      0.91      0.79       129
           1       0.88      0.62      0.72        81
           2       0.65      0.55      0.59       104
           3       0.62      0.72      0.67       130
           4       0.68      0.81      0.74       218
           5       0.00      0.00      0.00        39
           6       1.00      0.02      0.04        48
           7       0.45      0.40      0.42        93
           8       0.73      0.86      0.79       191

    accuracy                           0.67      1033
   macro avg       0.63      0.54      0.53      1033
weighted avg       0.66      0.67      0.65      1033

